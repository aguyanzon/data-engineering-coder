{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7bcd6e-97fb-4d2c-ba35-a50021ab82d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import requests\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, lit, col, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7317e671-5407-4cb0-aa9e-f4a49a3d9bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c8b346-7856-49a4-96df-19569fc6f53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Postgres and Redshift JDBCs\n",
    "driver_path = \"/home/coder/working_dir/driver_jdbc/postgresql-42.2.27.jre7.jar\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--driver-class-path {driver_path} --jars {driver_path} pyspark-shell'\n",
    "os.environ['SPARK_CLASSPATH'] = driver_path\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Conexion entre Pyspark y Redshift\") \\\n",
    "        .config(\"spark.jars\", driver_path) \\\n",
    "        .config(\"spark.executor.extraClassPath\", driver_path) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9055fb-e10d-4a46-9fde-4e87ce89d5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b6e2a6-a304-4376-ad61-180648d79485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to Redshift using psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host=env['AWS_REDSHIFT_HOST'],\n",
    "    port=env['AWS_REDSHIFT_PORT'],\n",
    "    dbname=env['AWS_REDSHIFT_DBNAME'],\n",
    "    user=env['AWS_REDSHIFT_USER'],\n",
    "    password=env['AWS_REDSHIFT_PASSWORD']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee260795-e5e3-4b5d-8863-4d3eb08648a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created!\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {env['AWS_REDSHIFT_SCHEMA']}.finance (\n",
    "            \"date_from\" VARCHAR(10),\n",
    "            \"1. open\" VARCHAR(10),\n",
    "            \"2. high\" VARCHAR(10),\n",
    "            \"3. low\" VARCHAR(10),\n",
    "            \"4. close\" VARCHAR(10), \n",
    "            \"5. volume\" VARCHAR(10),\n",
    "            symbol VARCHAR(10) distkey\n",
    "        ) sortkey(date_from);\n",
    "    ''')\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "print(\"Table created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d3d8dc-1241-4d9b-ab53-ab56f06baa59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_data(symbol):\n",
    "    try:\n",
    "        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY&symbol={symbol}&apikey={env[\"API_KEY\"]}'\n",
    "        response = requests.get(url)\n",
    "        json_data = response.json()\n",
    "\n",
    "        data_list = []\n",
    "        for date, values_dict in json_data[\"Monthly Time Series\"].items():\n",
    "            data = (date, values_dict[\"1. open\"], values_dict[\"2. high\"], values_dict[\"3. low\"], values_dict[\"4. close\"], values_dict[\"5. volume\"])\n",
    "            data_list.append(data)\n",
    "\n",
    "        # Crear el DataFrame con todos los datos\n",
    "        df = spark.createDataFrame(data_list, [\"date_from\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\"])\n",
    "        df = df.withColumn(\"symbol\", lit(symbol))\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error de solicitud: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0549286-00d8-4142-adb0-850168327316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_ibm = extract_data('IBM')\n",
    "data_aapl = extract_data('AAPL')\n",
    "data_tsla = extract_data('TSLA')\n",
    "data = data_ibm.union(data_aapl).union(data_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfdb63f5-fbe0-4bfd-abfc-8239215aa548",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_from: string (nullable = true)\n",
      " |-- 1. open: string (nullable = true)\n",
      " |-- 2. high: string (nullable = true)\n",
      " |-- 3. low: string (nullable = true)\n",
      " |-- 4. close: string (nullable = true)\n",
      " |-- 5. volume: string (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      "\n",
      "+----------+--------+--------+--------+--------+---------+------+\n",
      "| date_from| 1. open| 2. high|  3. low|4. close|5. volume|symbol|\n",
      "+----------+--------+--------+--------+--------+---------+------+\n",
      "|2023-06-12|128.4400|136.6200|127.7800|136.4200| 35186180|   IBM|\n",
      "|2023-05-31|126.3500|130.0699|120.5500|128.5900| 95710890|   IBM|\n",
      "|2023-04-28|130.9700|132.6100|124.5600|126.4100| 83664114|   IBM|\n",
      "|2023-03-31|128.9000|131.4800|121.7100|131.0900|138093084|   IBM|\n",
      "|2023-02-28|134.4900|137.3900|128.8600|129.3000| 76080679|   IBM|\n",
      "|2023-01-31|141.1000|147.1800|132.9800|134.7300|105576019|   IBM|\n",
      "|2022-12-30|149.9800|153.2100|137.1950|140.8900| 86426226|   IBM|\n",
      "|2022-11-30|138.2500|150.4600|133.9700|148.9000| 93620235|   IBM|\n",
      "|2022-10-31|120.1600|138.8615|115.5450|138.2900|113480787|   IBM|\n",
      "|2022-09-30|128.4000|130.9900|118.6100|118.8100| 87256958|   IBM|\n",
      "|2022-08-31|130.7500|139.3400|128.4000|128.4500| 77392342|   IBM|\n",
      "|2022-07-29|141.0000|141.8700|125.1300|130.7900|129801061|   IBM|\n",
      "|2022-06-30|139.6700|144.7300|132.8500|141.1900|105815743|   IBM|\n",
      "|2022-05-31|133.0000|139.8300|125.8000|138.8400|113207659|   IBM|\n",
      "|2022-04-29|129.6600|141.8800|124.9100|132.2100|107525264|   IBM|\n",
      "|2022-03-31|122.6700|133.0800|120.7000|130.0200| 96447210|   IBM|\n",
      "|2022-02-28|133.7600|138.8200|118.8100|122.5100| 98492968|   IBM|\n",
      "|2022-01-31|134.0700|142.2000|124.1930|133.5700|147238382|   IBM|\n",
      "|2021-12-31|118.2500|134.9900|116.5600|133.6600|113930079|   IBM|\n",
      "|2021-11-30|125.0500|127.2900|114.5600|117.1000|119252012|   IBM|\n",
      "+----------+--------+--------+--------+--------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b0f70e-b491-41ba-9de4-3a07d319e787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame no tiene duplicados.\n"
     ]
    }
   ],
   "source": [
    "# Verificación de duplicados\n",
    "# Verificar si el DataFrame tiene duplicados\n",
    "total_rows = data.count()\n",
    "distinct_rows = data.dropDuplicates().count()\n",
    "\n",
    "# Comparar la cantidad de filas antes y después de eliminar los duplicados\n",
    "if total_rows == distinct_rows:\n",
    "    print(\"El DataFrame no tiene duplicados.\")\n",
    "else:\n",
    "    print(\"El DataFrame tiene duplicados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1670d404-884a-4fad-9be2-8b7e78f38e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{env['AWS_REDSHIFT_HOST']}:{env['AWS_REDSHIFT_PORT']}/{env['AWS_REDSHIFT_DBNAME']}\") \\\n",
    "    .option(\"dbtable\", f\"{env['AWS_REDSHIFT_SCHEMA']}.finance\") \\\n",
    "    .option(\"user\", env['AWS_REDSHIFT_USER']) \\\n",
    "    .option(\"password\", env['AWS_REDSHIFT_PASSWORD']) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df933adb-a094-4d2a-84a4-fb973ecf44f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query Redshift using Spark SQL\n",
    "query = f\"select distinct symbol from {env['AWS_REDSHIFT_SCHEMA']}.finance\"\n",
    "data = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{env['AWS_REDSHIFT_HOST']}:{env['AWS_REDSHIFT_PORT']}/{env['AWS_REDSHIFT_DBNAME']}\") \\\n",
    "    .option(\"dbtable\", f\"({query}) as tmp_table\") \\\n",
    "    .option(\"user\", env['AWS_REDSHIFT_USER']) \\\n",
    "    .option(\"password\", env['AWS_REDSHIFT_PASSWORD']) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d205da-b2f9-45c1-b276-1e80dc060f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|symbol|\n",
      "+------+\n",
      "|   IBM|\n",
      "|  AAPL|\n",
      "|  TSLA|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54516)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298509c7-0b2e-40a0-93e4-c8dad679f51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
